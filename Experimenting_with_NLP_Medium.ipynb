{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elianedb/BERT_SE/blob/main/Experimenting_with_NLP_Medium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW70HOO55Vfw"
      },
      "source": [
        "# Practicing some Hugging Face Transformers Code\n",
        "This notebook goes through a lightning quick demonstration of some of the cutting-edge language models that are available, open source, to anyone with an internet connection. The organisation HuggingFace (https://huggingface.co/) has made them super easy to use, so feel free to play around with the inputs here if you want to see how these work.\n",
        "\n",
        "This code comes from https://huggingface.co/transformers/task_summary.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR7DVvvSg5b_"
      },
      "source": [
        "## Running the code in this notebook\n",
        "\n",
        "Running the code in this notebook is really easy. As you run your mouse over a block of code on the left hand side of each block you'll see a small play button - just click the play button to run the code. You'll see a whirling circle; that means the code's running. Leave it to finish running and then run the next block of code. Usually you'll see some output below the block of code. Each block will take a few seconds to run as you're running this code on some Google servers somewhere.\n",
        "\n",
        "First, we need to load up the library that runs these models as they aren't pre-loaded onto Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2DMuvI05oeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "107af7ab-6cf0-44a6-d3b8-f0360b9e5780"
      },
      "source": [
        "!pip install -q transformers\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 41.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 50.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCell_jKXTds"
      },
      "source": [
        "# Paraphrase Detection\n",
        "This nifty pre-trained model has been trained to classify whether two sentences are paraphrases of each other. We start off by loading the pre-trained model. This model has been **fine-tuned** on this task, meaning that after the initial model was trained to do it's original (more general task), it was then trained with a data set comprising examples of paraphrased sentences. This is a great example of **transfer learning**.\n",
        "\n",
        "Don't worry if you get what looks like a warning after running this code. You can ignore it, the model's good to go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI5MgUC05n4G"
      },
      "source": [
        "# Import tokenizer (thing that turns words to number for the model) and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "# Setup the two classed for the model\n",
        "classes = [\"not a paraphrase\", \"a paraphrase\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvYPvz_82hNK"
      },
      "source": [
        "## Giving the model some examples\n",
        "Below are a few examples we'll feed to the model to see how good it is at detecting whether something is a paraphrase or not. Go ahead and change the bits of text between the quotations marks \"\" to see if you can fool the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFuT_sbZ2OOL"
      },
      "source": [
        "sequence_1 = \"Rick and Morty is my favourite show on Netflix\"\n",
        "sequence_2 = \"I love building machine learning models to understand language\"\n",
        "sequence_3 = \"Natural Language Processing is one of my favourite areas of machine learning\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsK1howi5iRX"
      },
      "source": [
        "# Tokenize the sentences, turning them into a numerical representation the model can read\n",
        "paraphrase = tokenizer(sequence_1, sequence_3, return_tensors=\"tf\")\n",
        "not_paraphrase = tokenizer(sequence_2, sequence_3, return_tensors=\"tf\")\n",
        "\n",
        "# Let's get the model's predictions\n",
        "paraphrase_predictions = tf.nn.softmax(model(paraphrase)[0], axis =1).numpy()[0]\n",
        "not_paraphrase_predictions = tf.nn.softmax(model(not_paraphrase)[0], axis = 1).numpy()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICfIX9UtYMe6"
      },
      "source": [
        "Now, let's take a look at how good it is at picking up whether one sentence is a paraphrase of another"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORihuNKg6tvv"
      },
      "source": [
        "# Let's take a look at the results from the 1st and 3rd sentences\n",
        "print(\" === Example one ===\")\n",
        "print(\"Sentence 1: {} \\nSentence 2: {}\".format(sequence_1, sequence_3))\n",
        "print(\"\\nModel prediction:\")\n",
        "for i in range(len(classes)):\n",
        "  print(\"Probability it's {}: {}%\".format(classes[i], round(paraphrase_predictions[i]*100,2)))\n",
        "\n",
        "# And now from the the 2nd and 3rd sentences\n",
        "print(\"\\n === Example Two ===\")\n",
        "print(\"Sentence 2: {} \\nSentence 3: {}\".format(sequence_2, sequence_3))\n",
        "print(\"\\nModel prediction:\")\n",
        "for i in range(len(classes)):\n",
        "  print(\"Probability it's {}: {}%\".format(classes[i], round(not_paraphrase_predictions[i]*100,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J42jOHx57wly"
      },
      "source": [
        "# Transformers Q&A \n",
        "The ease with which one can build question and answering (Q&A) models blows my mind. Basically, the way the model works is by finding the most likely start and end token (word) positions based on the question asked. The below code utilises a pre-trained model called BERT for Q&A. \n",
        "\n",
        "Code taken from https://huggingface.co/transformers/task_summary.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn1qjRXJ-zMU"
      },
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5it36YkL7N_B"
      },
      "source": [
        "Below we load the fine-tuned model. Don't worry if you some warning messages; the model is good to go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SEE0YAn-4F8"
      },
      "source": [
        "# We load up the pretrained tokenizer and model, both trained/fine-tuned on a QA task\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "qa_model = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-n9DjPIhuwZ"
      },
      "source": [
        "To make this task more real, I went and copied the first paragraph of the Wikipedia article on Apple computers and then thought of two questions to ask. After you see how this model works, there's a code block below for you to enter in your own text and the questions you want to the model to try answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDfijm69_FGm"
      },
      "source": [
        "# Add some text and questions\n",
        "text = r\"\"\"Apple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976 to develop and sell Wozniak's Apple I personal computer, \n",
        "           though Wayne sold his share back within 12 days. It was incorporated as Apple Computer, Inc., in January 1977, and sales of its computers, \n",
        "           including the Apple II, grew quickly. Within a few years, Jobs and Wozniak had hired a staff of computer designers and had a production line. \n",
        "           Apple went public in 1980 to instant financial success. Over the next few years, Apple shipped new computers featuring innovative graphical \n",
        "           user interfaces, such as the original Macintosh in 1984, and Apple's marketing advertisements for its products received widespread critical \n",
        "           acclaim. However, the high price of its products and limited application library caused problems, as did power struggles between executives. \n",
        "           In 1985, Wozniak departed Apple amicably and remained an honorary employee,[9] while Jobs and others resigned to found NeXT.[10]\n",
        "          As the market for personal computers expanded and evolved through the 1990s, Apple lost market share to the lower-priced duopoly of Microsoft \n",
        "          Windows on Intel PC clones. The board recruited CEO Gil Amelio to what would be a 500-day charge for him to rehabilitate the financially troubled \n",
        "          company—reshaping it with layoffs, executive restructuring, and product focus. In 1997, he led Apple to buy NeXT, solving the desperately \n",
        "          failed operating system strategy and bringing Jobs back. Jobs pensively regained leadership status, becoming CEO in 2000. Apple swiftly \n",
        "          returned to profitability under the revitalizing Think different campaign, as he rebuilt Apple's status by launching the iMac in 1998, \n",
        "          opening the retail chain of Apple Stores in 2001, and acquiring numerous companies to broaden the software portfolio. In January 2007, \n",
        "          Jobs renamed the company Apple Inc., reflecting its shifted focus toward consumer electronics, and launched the iPhone to great critical \n",
        "          acclaim and financial success. In August 2011, Jobs resigned as CEO due to health complications, and Tim Cook became the new CEO. \n",
        "          Two months later, Jobs died, marking the end of an era for the company. \n",
        "      \"\"\"\n",
        "\n",
        "questions = [\"Who founded Apple?\",\n",
        "             \"Where did Apple lose market share?\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewwTs-bmh5Fe"
      },
      "source": [
        "Let's see how the model does..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL8nXRw__aw1"
      },
      "source": [
        "def answer_question(question, text):\n",
        "  # Function that simplifies answering a question\n",
        "  for question in questions:\n",
        "    # Concatenate the question and the textx\n",
        "    inputs = tokenizer(question, text, add_special_tokens = True, return_tensors = 'tf')\n",
        "    # Get the input ids (numbers) and convert to tokens (words)\n",
        "    input_ids = inputs[\"input_ids\"].numpy()[0]\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    # Run the pretrained model to get the logits (raw scores) for the scores\n",
        "    output = qa_model(inputs)\n",
        "\n",
        "    # Get the most likely beginning and end\n",
        "    answer_start = tf.argmax(output.start_logits, axis = 1).numpy()[0]\n",
        "    answer_end = (tf.argmax(output.end_logits, axis = 1)+1).numpy()[0]\n",
        "    # Turn the tokens from the ids of the input string, indexed by the start and end tokens back into a string\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    print(\"Question {} \\nAnswer: {}\".format(question, answer))\n",
        "\n",
        "answer_question(questions, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBqBAE3e60q_"
      },
      "source": [
        "## Play around with the model yourself\n",
        "You can edit the text and the questions below to see how the model answers your own questions to your own piece of text. Make sure to keep the format as it is below though, with three quotations marks \"\"\" on either side of your **text** and your **questions** within the single quotes within the square brackets [ ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwnlwkxILwJs"
      },
      "source": [
        "text = r\"\"\"Enter in the text you want the AI to find answers in here \"\"\"\n",
        "questions = [\"Type question 1 here\",\n",
        "             \"Type question 2 here or leave blank\"]\n",
        "\n",
        "answer_question(questions, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjlY9Ig49v8S"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}