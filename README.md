# BERT_SE

BERT_SE is a BERT model trained on a textual dataset in the area of software engineering.

BERT_SE went through a fine-tuning process, using the algorithms provided by the BERT authors [1]. The dataset used in fine-tuning, called corp_SE, is composed of texts obtained from Stackoverflow issues and user requirements. In addition, it composes the dataset, software requirements obtained from open-source projects. The corp_SE it's composed of 319.026 requirements from 16 large open-source projects in 9 repositories (Apache, Appcelerator, DuraSpace, Atlassian, Moodle, Lsstcorp, Mulesoft, Spring and Talendforge) [1] and from others 22 open-source datasets [2].

Therefore, the corp_SE is composed of 456.500 texts, in this paper called sentences. Each sentence has an average length of 61 words. The vocabulary generated by the corp_SE is composed of 1.179.501 words.

The BERT_SE is available in this repository for use in software engineering tasks. It results and details of the evaluation are in the paper. 


References:

[1] M. Choetkiertikul, HK Dam, T. Tran, TTM Pham, A. Ghose e T. Menzies, "A deep learning model for estimating story points.", IEEE Trans. Softw. Eng. Vol. PP, n√£o. 99, p. 1, 2018.

[2] F. Dalpiaz, A. Ferrari, X. Franch, and C. Palomares. "Natural language processing for requirements engineering: The best is yet to come." IEEE software 35, no. 5 (2018): 115-119.

[3] J. Devlin, M. Chang, K. Lee, and K. Toutanova. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018).
